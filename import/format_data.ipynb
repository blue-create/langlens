{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BbjDacn_sZj"
      },
      "source": [
        "# Purpose\n",
        "\n",
        "This file shows the steps we took to process the raw data files, zipped xml files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSBVVOc8_6DR"
      },
      "source": [
        "### Connect with Google drive to access data\n",
        "\n",
        "In order to access the data, you first need to create a shortcut of the data folder to your own Gdrive. If you've been granted editing rights, you should be able to edit the content of the folder, i.e. add, move and delete data, create and rename folders, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55fsThdDRuDs",
        "outputId": "ac4792ec-fc1e-4f02-d592-15e2b38af95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdHvzLHiXTk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb19069-d750-4ec3-a785-0029716cb0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ],
      "source": [
        "# redirect the working directory of this script to the data folder\n",
        "%cd /content/drive/MyDrive/data/\n",
        "#%cd /content/drive/MyDrive/Work/Frontline/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDLAdk9nWvNY",
        "outputId": "e07e709f-b1e1-4ded-9b54-91c7c7f873c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the folder:  257\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# check number of files \n",
        "num_files = len(os.listdir(\"Raw\"))\n",
        "print(\"Number of files in the folder: \", num_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqh8TaqvAayx"
      },
      "source": [
        "## Unzip the files \n",
        "\n",
        "The first step was to unzip the files and to move the unzipped xml files to another folder called \"unzipped\". Next, we check whether any zip file in the \"raw\" data folder is missing a corresponding xml file in the \"unzipped\" folder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6QhI0PoVFUk"
      },
      "outputs": [],
      "source": [
        "# unzip all files \n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Path to the raw folder\n",
        "raw_folder = \"./Raw\"\n",
        "\n",
        "# Path to the unzipped folder\n",
        "unzipped_folder = \"./unzipped\"\n",
        "\n",
        "# Loop through all the files in the raw folder\n",
        "for filename in os.listdir(\"./Raw\")[-4:]:#os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Check if the file is a valid zip file\n",
        "        if zipfile.is_zipfile(os.path.join(raw_folder, filename)):\n",
        "            # Create a ZipFile object for the current zip file\n",
        "            with zipfile.ZipFile(os.path.join(raw_folder, filename), \"r\") as zip_ref:\n",
        "                # Extract all the contents of the zip file to the unzipped folder\n",
        "                zip_ref.extractall(unzipped_folder)\n",
        "        else:\n",
        "            print(f\"File {filename} is not a valid zip file and will not be extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS_uVeg2pl1G"
      },
      "outputs": [],
      "source": [
        "# check whether a zip file is missing a xml file \n",
        "\n",
        "# Loop through all the files in the Raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Get the first part of the filename before the .zip extension\n",
        "        name = filename.split(\".\")[0]\n",
        "\n",
        "        # Loop through all the files in the unzipped folder\n",
        "        for xml_filename in os.listdir(unzipped_folder):\n",
        "            if xml_filename.startswith(name) and xml_filename.endswith(\".xml\"):\n",
        "                break\n",
        "        else:\n",
        "            # A corresponding XML file doesn't exist\n",
        "            print(f\"No corresponding XML file exists for {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWH914I31Vhg"
      },
      "source": [
        "## Reading XMLS and filtering\n",
        "In this step all prefixes are looped through. Before parsing the xmls files, it is checked if a csv file for that prefix already exists. If not, all the XMls with that prefix are read and filtered. If they contain words/ phrases associated with domestic violence, they are exported as csv, using their prefix as name. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8A0YlHZF2Bt"
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W6P7pHlGBPY"
      },
      "outputs": [],
      "source": [
        "FILTERED_PATH=\"filtered_4_28\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpPex4jIbqP_"
      },
      "outputs": [],
      "source": [
        "comb2=['häuslicher? gewalt',\n",
        "        'partnerschaftsgewalt',\n",
        "        'partnergewalt',\n",
        "        'femizid',\n",
        "        'beziehungstat',\n",
        "        'liebesdrama',\n",
        "        'ehedrama',\n",
        "        'liebestragödie',\n",
        "        'eheliche gewalt',\n",
        "        'ehekrieg',\n",
        "        'innerfamiliäre gewalt',\n",
        "        'innerhäusliche gewalt',\n",
        "        '\\bgewalt\\b und \\behe\\b',\n",
        "        '\\behe\\b und hölle',\n",
        "       \"vergewaltigt und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"missbraucht und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"übergriffig und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gewalttätig und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "      # \"verletzt und (freund(in)?|partner(in)?|mann|frau)\\s\", \n",
        "       \"\\b(ge)?schlagen\\b und (freund(in)?|partner(in)?|mann|frau)\\s\", \n",
        "       \"missbrauchen und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"missbrauch und (freund(in)?|partner(in)?|mann|frau)\\s\", \n",
        "       \"verletzung und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gedroht und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gezwungen und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"bedroht und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"beleidigt und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"beleidigung und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gaslighting und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"manipuliert und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"manipulieren und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gezwungen und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"eingesperrt und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"gestalkt und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "       \"einsperren und (freund(in)?|partner(in)?|mann|frau)\\s\",\n",
        "        'stalken und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'kontrollieren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'kontrolliert und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'isolieren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'isoliert und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        '\\bhauen\\b und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'ohrfeige und (freund(in)?|partner(in)?|mann|frau)\\s'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRyLzbCkXa6M"
      },
      "outputs": [],
      "source": [
        "comb1 = ['häuslicher? gewalt',\n",
        "        'partnerschaftsgewalt',\n",
        "        'partnergewalt',\n",
        "        'femizid',\n",
        "        'beziehungstat',\n",
        "        'liebesdrama',\n",
        "        'ehedrama',\n",
        "        'liebestragödie',\n",
        "        'eheliche gewalt',\n",
        "        'ehekrieg',\n",
        "        'innerfamiliäre gewalt',\n",
        "        'innerhäusliche gewalt',\n",
        "        'gewalt und \\behe\\b',\n",
        "        '\\behe\\b und hölle'\n",
        "        'gewalt und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'vergewaltigen und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'vergewaltigung und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'missbrauch und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'missbräuchlich und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'gewalttätig und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'verletzung und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'verletzen und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'übergriffig und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'drohung und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'drohen und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'manipulation und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'manipulieren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'beleidigen und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'beleidigung und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'gaslighting und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        '\\b(ge)?schlagen\\b und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'zwingen und (freund(in)?|partner(in)?|mann|frau)\\s', \n",
        "        'gezwungen und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'zwang und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'einsperren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'stalking und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'stalken und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'kontrollieren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'kontrolle und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'isolieren und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'isolation und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        '\\bhauen\\b und (freund(in)?|partner(in)?|mann|frau)\\s',\n",
        "        'ohrfeige und (freund(in)?|partner(in)?|mann|frau)\\s'\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGsImkzwk_bF"
      },
      "outputs": [],
      "source": [
        "comb3=[\n",
        "    \"häuslicher? gewalt\",\n",
        "    \"ehedrama\",\n",
        "    \"familienstreit und partner(in)?\",\n",
        "    \"gewalttätige[rn]? (ex-)?partner(in)?\",\n",
        "    \"gewalttätige[rn]? Ehe(partner|mann|frau)?(in)?\", #  -> ok\n",
        "    \"partnerschaftsgewalt\",# -> good\n",
        "    \"femizid\",# -> ok\n",
        "    \"gewalt in (der )?(\\(ex-\\))?(ex)?partnerschaft(en)?\",\n",
        "    \"beziehungsgewalt\",# -> good\n",
        "    \"beziehungstat\",\n",
        "    \"gewalttätige[rn]? (ex(-)?)?freund(in)?\",\n",
        "    \"beziehungsdrama\",# -> good\n",
        "# gewalt gegen (männer|frauen) -> not good, politics\n",
        "# partnergewalt c häusliche gewalt\n",
        "# bluttat und partner(in)? -> not\n",
        "# frauenhaus -> ok\n",
        "# liebesdrama, tragödie -> not good, books\n",
        "# eheliche gewalt -> no hits\n",
        "# familiäre gewalt -> not good\n",
        "# ehekrieg -> not good\n",
        "# beziehungskrieg -> not good\n",
        "]\n",
        "\n",
        "comb_no_match=[\"filme?\",\"putin\",\"schauspielbühnen?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF1yF5Tr3ev0"
      },
      "outputs": [],
      "source": [
        "\n",
        "exclude_titles=[\n",
        "  #LESERBRIEFE\n",
        "  \"lesermeinung\",\n",
        "  \"leserbrief\",\n",
        "  \"LIEBE LESERIN, LIEBER LESER\",\n",
        "  #INTERVIEWS\n",
        "  #\"interview\",\n",
        "\n",
        "  #ANDERE KATEGORIEN\n",
        "  \"Corona-Ticker\"\n",
        "  \"EM-Tabelle\",\n",
        "  \"finanztrends\",\n",
        "  \"News Ticker\",\n",
        "  # NOTDIENSTE, HILFEKONTAKTE\n",
        "  #\"HIER FINDEN SIE HILFE\",\n",
        "  #\"notdienste\",\n",
        "  #\"Rat & Hilfe\",\n",
        "  #\"WAS, WANN, WO\"\n",
        "  #\"Was - wann -wo\"\n",
        "  #\"Hier_finden_Sie_Hilfe\"\n",
        "  #TV PROGRAMM, \"BÜCHER\"\n",
        "  \"TV-Programm\",\n",
        "  \"TV-\"\n",
        "  \"Kriminalhörspiel\",\n",
        "  \"TV-Tipp des Tages\",\n",
        "  \"thriller\"\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDNYSPFcobQ"
      },
      "source": [
        "### Filtering and Exporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QqNHp-uCmEm"
      },
      "outputs": [],
      "source": [
        "# load packages and set directories \n",
        "import os \n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET \n",
        "from tqdm import tqdm # for progress bar \n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set up paths\n",
        "input_dir = \"unzipped\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbSGEA1tG5RR"
      },
      "source": [
        "#### Methods\n",
        "For this step, the following methods are defined:\n",
        "* parsing xml files\n",
        "* testing if csv was exported already\n",
        "* checking if text contains DV relatedwords\n",
        "* check if string ocurs in text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7NRwFGc6Bsh"
      },
      "outputs": [],
      "source": [
        "def parse_xml_file(xml_file):\n",
        "  \"\"\" function to combine all xml files of a prefix and returns it as json\n",
        "  Parameters:\n",
        "    - prefix (str): prefix of the journal \n",
        "  Returns:\n",
        "    - json: containing the combined xml files of a prefix as list\n",
        "  \"\"\"\n",
        "  tree = ET.parse(os.path.join(input_dir, xml_file))\n",
        "  # Get the root \n",
        "  root = tree.getroot()\n",
        "  # Create list for converted output \n",
        "  json_file = []\n",
        "  # Loop through each article, get the data and append it to the output file json_file\n",
        "  for artikel in root.findall('artikel'):\n",
        "    # Access static data by their xpath\n",
        "    # Store data unless data is not available and None, then store as None \n",
        "    artikel_id = artikel.find('metadaten/artikel-id')\n",
        "    artikel_id = artikel_id.text if artikel_id is not None else None \n",
        "\n",
        "    name = artikel.find('metadaten/quelle/name').text\n",
        "\n",
        "    jahrgang = artikel.find('metadaten/quelle/jahrgang')\n",
        "    jahrgang = jahrgang.text if jahrgang is not None else None\n",
        "\n",
        "    datum = artikel.find('metadaten/quelle/datum')\n",
        "    datum = datum.text if datum is not None else None\n",
        "\n",
        "\n",
        "    # Access variable data by their xpath \n",
        "    ressort_elem = artikel.find('inhalt/titel-liste/ressort')\n",
        "    # Store data unless data is not available and None, then store as None \n",
        "    ressort = ressort_elem.text if ressort_elem is not None else None \n",
        "\n",
        "    titel_elem = artikel.find('inhalt/titel-liste/titel')\n",
        "    titel = titel_elem.text if titel_elem is not None else None \n",
        "\n",
        "    untertitel_elem = artikel.find('inhalt/titel-liste/untertitel')\n",
        "    untertitel = untertitel_elem.text if untertitel_elem is not None else None\n",
        "\n",
        "    # Create list for text inputs \n",
        "    text = []\n",
        "    # Find the 'text' element\n",
        "    text_elem = artikel.find('inhalt/text')\n",
        "    try: \n",
        "        # Extract all the 'p' elements inside the 'text' element\n",
        "        p_elems = text_elem.findall('p')\n",
        "        # Loop over the 'p' elements and extract their text content\n",
        "        for p_elem in p_elems:\n",
        "            p_text = p_elem.text\n",
        "            # Only add text if text is not empty \n",
        "            if p_text is not None: \n",
        "              text.append(p_text)\n",
        "\n",
        "    # If no text element exists, pass \n",
        "    except: \n",
        "        pass \n",
        "\n",
        "    # Create temporary dict to store all information \n",
        "    temp_dict = {}\n",
        "    temp_dict['artikel_id'] = str(artikel_id)\n",
        "    temp_dict['name'] = name\n",
        "    temp_dict['jahrgang'] = jahrgang\n",
        "    temp_dict['datum'] = datum\n",
        "    temp_dict['ressort'] = ressort\n",
        "    temp_dict['titel'] = titel\n",
        "    temp_dict['untertitel'] = untertitel\n",
        "    temp_dict['text'] = text\n",
        "\n",
        "    # Add the article dict to the output list \n",
        "    json_file.append(temp_dict)\n",
        "  return json_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugnHlDF1Z0VO"
      },
      "outputs": [],
      "source": [
        "def filter_text_by_list(text, list_match, list_no_match):\n",
        "  \"\"\" function to compare if any of a list of words occur in a text or or list of texts \n",
        "  Parameters:\n",
        "    - text (str or list of str): text which is checked \n",
        "    - list_match (list of str): list of words which are checked if they occur in text \n",
        "    - list_no_match (list of str): list of words that should not occur in the text\n",
        "  Returns:\n",
        "    - boolean: True or False depending on if any of the words in list_match occurs in text\n",
        "  \"\"\"\n",
        "  text=\" \".join(text).lower()\n",
        "\n",
        "  for comb in list_match:\n",
        "    if all(occurs(word,text)  for word in comb.split(\" und \")):\n",
        "        for comb_not in list_no_match:\n",
        "          if occurs(comb_not, text):\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJBbe6OeW5SQ"
      },
      "outputs": [],
      "source": [
        "def check_if_exported(prefix, filtered_path):\n",
        "  \"\"\" function to check if the relevant articles of a journal have been exported already\n",
        "  Parameters:\n",
        "    - prefix (str): prefix of the journal that is checked\n",
        "  Returns:\n",
        "    - boolean: returns True if the relevant articles of a journal have been exported already, False otherwise\n",
        "  \"\"\"\n",
        "  filtered=os.listdir(filtered_path)\n",
        "  filtered = [file.split(\".\")[0]for file in filtered if file.endswith(\".csv\")]\n",
        "  if prefix in filtered:\n",
        "    print(f\"{prefix} csv already exported\")\n",
        "    return True\n",
        "  else: \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5cECPQsPLLn"
      },
      "outputs": [],
      "source": [
        "def occurs(word, text):\n",
        "  \"\"\" function to check if a words occurs in a text\n",
        "  Parameters:\n",
        "    - word (str): word that is searched for\n",
        "    - text (str): text that is searched in \n",
        "  Returns:\n",
        "    - boolean: returns True if word occurs in text, False otherwise\n",
        "  \"\"\"\n",
        "  if len(re.findall(word,text))>0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EnfcxByqmjR"
      },
      "outputs": [],
      "source": [
        "def filter_title(title):\n",
        "  \"\"\" function to filter article by title\n",
        "  Parameters:\n",
        "    - title (str): title that is checked\n",
        "  Returns:\n",
        "    - boolean: returns True if either\n",
        "        - there is no title\n",
        "        - the title does not contain any of the words in the list exclude_titles\n",
        "  \"\"\"\n",
        "  if type(title)!=str:\n",
        "    return True\n",
        "  for ex in exclude_titles:\n",
        "    if ex.lower() in title.lower(): \n",
        "      return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptRgTeZL_4vm"
      },
      "source": [
        "#### Filtering on Country\n",
        "In this step all journals that are not Germany-based are excluded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-d_qn9M_2yR"
      },
      "outputs": [],
      "source": [
        "# create sorted list of all prefixes \n",
        "all_prefixes= sorted([i.split(\".\")[0] for i in os.listdir(\"Raw\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMwwgkXqAOyl",
        "outputId": "c775aa03-dbb9-499f-a762-62b4d26ed30b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 257/257 [08:04<00:00,  1.89s/it]\n"
          ]
        }
      ],
      "source": [
        "# create a dictionary with all prefixes and their corresponding journal names\n",
        "all_journals={}\n",
        "\n",
        "# loop through all prefixes \n",
        "for prefix in tqdm(all_prefixes):\n",
        "  # list all xmls of a prefix\n",
        "  for xml in os.listdir(input_dir):\n",
        "    if xml.startswith(prefix+\"_\"):\n",
        "      name=parse_xml_file(xml)[0][\"name\"]\n",
        "      all_journals[prefix]=name\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCcdaiOWE7FN"
      },
      "outputs": [],
      "source": [
        "# prefixes of journals from Switzerland, Austria, etc.\n",
        "prefixes_to_exclude=['AGZ', \"ANEW\", \"APZT\",\"AWP\", \"AWPO\", \"AWPU\", \"BAZ\", \"BERN\", \"BEOB\", \"BLI\", \"BUND\", \"BWAI\", \"DIEW\", \"DOL\", \"ELNA\",\"HZ\", \"HZO\", \"KLEI\", \"KRON\", \"KUR\", \"LUXT\", \"LZLZ\", \"NECH\",\"NLZ\", \"NVB\",\"NVT\",\"NZZ\", \"NZZS\", \"OOEN\", \"PBN\", \"PRE\", \"PROF\", \"RVZ\",\"SBLI\",\"SN\",\"STA\",\"STG\",\"TAG\",\"TAS\",\"THTA\",\"TITA\",\"VN\",\"WEWO\",\"WZ\",\"ZSAS\", \"NBPC\",\"NBPC_part1\",\"NBPC_part2\",\"NBPC_part3\",\"NBPC_part4\",\"FALT\",\"RTAL\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soKTh8Rn2w5I"
      },
      "outputs": [],
      "source": [
        "# list with all prefixes\n",
        "prefixes= sorted([i.split(\".\")[0] for i in os.listdir(\"Raw\")])\n",
        "# remove the non-German journals\n",
        "prefixes=list(set(prefixes) - set(prefixes_to_exclude))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsaZupmYAHl9"
      },
      "source": [
        "#### Filtering based on key-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my9Pn_kC66bV"
      },
      "outputs": [],
      "source": [
        "# looping through all prefixes\n",
        "for prefix in tqdm(prefixes):\n",
        "  #check if the prefixes articles were exported already\n",
        "  if not check_if_exported(prefix, FILTERED_PATH):\n",
        "    # list all xmls of a prefix\n",
        "    xmls=[i for i in os.listdir(input_dir) if i.startswith(prefix+\"_\")]\n",
        "    # create an empty list for the json files\n",
        "    DV_art=[]\n",
        "    \n",
        "    # loop through all xmlfile\n",
        "    for xml in xmls:\n",
        "      # parse each xml\n",
        "      parsed_xml=parse_xml_file(xml)\n",
        "      \n",
        "      #loop through all articles  \n",
        "      for art in parsed_xml:\n",
        "        if filter_title(art[\"titel\"] ):\n",
        "        #check if articles are associated with DV\n",
        "          if filter_text_by_list(art[\"text\"],comb3, comb_no_match):\n",
        "              # DV articles are added to a list\n",
        "              \n",
        "              DV_art.append(art) \n",
        "    \n",
        "    DV_art=pd.DataFrame(DV_art)\n",
        "    #some articles show up multiple times with different id, remove those\n",
        "    DV_art=DV_art.drop_duplicates(\"text\") \n",
        "\n",
        "    DV_art.to_csv(FILTERED_PATH+\"/\"+prefix+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zUrg5NTFAop"
      },
      "source": [
        "#### Testing combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test filter words on a specific newspaper."
      ],
      "metadata": {
        "id": "sQAVnLtkY9zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get file names of files starting with BADZ_\n",
        "xmls=[i for i in os.listdir(input_dir) if i.startswith(\"BADZ_\")]"
      ],
      "metadata": {
        "id": "6--GQNl8Yfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSzGtexKMxIP",
        "outputId": "45da8f6c-0e6a-4ef5-e6b1-515b84d32fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:31<00:00,  2.08s/it]\n"
          ]
        }
      ],
      "source": [
        "# create an empty list for the json files\n",
        "\n",
        "DV_art=[]\n",
        "\n",
        "# loop through all xmlfile\n",
        "for xml in tqdm(xmls):\n",
        "  # parse each xml\n",
        "  parsed_xml=parse_xml_file(xml)\n",
        "  #loop through all articles  \n",
        "  for row_j in range(len(parsed_xml)):\n",
        "    #check if articles are associated with DV\n",
        "    if filter_title(parsed_xml[row_j][\"text\"]):\n",
        "      if filter_text_by_list(parsed_xml[row_j][\"text\"],comb3, comb_no_match):\n",
        "          # DV articles are added to a list\n",
        "          DV_art.append(parsed_xml[row_j])  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(DV_art)"
      ],
      "metadata": {
        "id": "jlIcFtxKcQ7R",
        "outputId": "7e8a9091-5432-40a4-b36b-981700d03871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "# Get a random subset of the list with 3 elements\n",
        "subset = random.sample(DV_art, 50)"
      ],
      "metadata": {
        "id": "XZf0rDAOZL64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVxlvaaZYC0u"
      },
      "outputs": [],
      "source": [
        "subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljJHggXrS2y"
      },
      "source": [
        "### Other possible filters:\n",
        "* exclude readers letters\n",
        "* exclude contact details for prevention help hotlines\n",
        "* exclude: Tatort, Wetter, Top & Flop\n",
        "*Leserbriefe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBwDGU-Jy-sC"
      },
      "source": [
        "## to do list:\n",
        "\n",
        "* do descriptive analysis: number of articles per newspaper, number of newspapers, number of topics per newspaper, etc.\n",
        "\n",
        "* do a collocation analysis: see gitub repo \"newspaper\" under scripts\n",
        "\n",
        "* topic analysis: run the filtered dataset through a generic topic model\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sqh8TaqvAayx",
        "ptRgTeZL_4vm"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

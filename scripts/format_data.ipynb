{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Sqh8TaqvAayx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose\n",
        "\n",
        "This file shows the steps we took to process the raw data files, zipped xml files."
      ],
      "metadata": {
        "id": "8BbjDacn_sZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect with Google drive to access data\n",
        "\n",
        "In order to access the data, you first need to create a shortcut of the data folder to your own Gdrive. If you've been granted editing rights, you should be able to edit the content of the folder, i.e. add, move and delete data, create and rename folders, etc."
      ],
      "metadata": {
        "id": "tSBVVOc8_6DR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiZS90YvWYBt"
      },
      "outputs": [],
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# redirect the working directory of this script to the data folder\n",
        "#%cd /content/drive/MyDrive/data/\n",
        "%cd /content/drive/MyDrive/Work/Frontline/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdHvzLHiXTk4",
        "outputId": "3aacd208-7ce3-4363-e636-7f1cbb92e23e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1WfnZsqpG1r110J63sMbfS5TpsDOkveiV/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# check number of files \n",
        "num_files = len(os.listdir(\"Raw\"))\n",
        "print(\"Number of files in the folder: \", num_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDLAdk9nWvNY",
        "outputId": "051b9639-49a9-456e-c3c8-0b12e426b4c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the folder:  253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the files \n",
        "\n",
        "The first step was to unzip the files and to move the unzipped xml files to another folder called \"unzipped\". Next, we check whether any zip file in the \"raw\" data folder is missing a corresponding xml file in the \"unzipped\" folder. "
      ],
      "metadata": {
        "id": "Sqh8TaqvAayx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip all files \n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Path to the raw folder\n",
        "raw_folder = \"./Raw\"\n",
        "\n",
        "# Path to the unzipped folder\n",
        "unzipped_folder = \"./unzipped\"\n",
        "\n",
        "# Loop through all the files in the raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Check if the file is a valid zip file\n",
        "        if zipfile.is_zipfile(os.path.join(raw_folder, filename)):\n",
        "            # Create a ZipFile object for the current zip file\n",
        "            with zipfile.ZipFile(os.path.join(raw_folder, filename), \"r\") as zip_ref:\n",
        "                # Extract all the contents of the zip file to the unzipped folder\n",
        "                zip_ref.extractall(unzipped_folder)\n",
        "        else:\n",
        "            print(f\"File {filename} is not a valid zip file and will not be extracted.\")"
      ],
      "metadata": {
        "id": "E6QhI0PoVFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether a zip file is missing a xml file \n",
        "\n",
        "# Loop through all the files in the Raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Get the first part of the filename before the .zip extension\n",
        "        name = filename.split(\".\")[0]\n",
        "\n",
        "        # Loop through all the files in the unzipped folder\n",
        "        for xml_filename in os.listdir(unzipped_folder):\n",
        "            if xml_filename.startswith(name) and xml_filename.endswith(\".xml\"):\n",
        "                break\n",
        "        else:\n",
        "            # A corresponding XML file doesn't exist\n",
        "            print(f\"No corresponding XML file exists for {filename}\")"
      ],
      "metadata": {
        "id": "LS_uVeg2pl1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert xml files to json files \n",
        "\n",
        "For further analysis, we decided to convert the xml files to json files for the following reasons: \n",
        "\n",
        "- we are dealing with big datasets \n",
        "- CSVs are slow to query and difficult to store efficiently \n",
        "- JSON supports more complex data structures"
      ],
      "metadata": {
        "id": "rAVGMAatBEUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Methods\n",
        "For this step, the following methods are defined:\n",
        "* parsing xml files\n",
        "* testing if exported files are complete\n"
      ],
      "metadata": {
        "id": "CbSGEA1tG5RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_xml_file(xml_file):\n",
        "  \"\"\" function to combine all xml files of a prefix and returns it as json\n",
        "  Parameters:\n",
        "    - prefix (str): prefix of the journal that is checked for completeness\n",
        "  Returns:\n",
        "    - json: containing the combined xml files of a prefix in json format\n",
        "  \"\"\"\n",
        "  tree = ET.parse(os.path.join(input_dir, xml_file))\n",
        "  # Get the root \n",
        "  root = tree.getroot()\n",
        "  # Create list for converted output \n",
        "  json_file = []\n",
        "  # Loop through each article, get the data and append it to the output file json_file\n",
        "  for artikel in root.findall('artikel'):\n",
        "    # Access static data by their xpath\n",
        "    # Store data unless data is not available and None, then store as None \n",
        "    artikel_id = artikel.find('metadaten/artikel-id')\n",
        "    artikel_id = artikel_id.text if artikel_id is not None else None \n",
        "\n",
        "    name = artikel.find('metadaten/quelle/name').text\n",
        "\n",
        "    jahrgang = artikel.find('metadaten/quelle/jahrgang')\n",
        "    jahrgang = jahrgang.text if jahrgang is not None else None\n",
        "\n",
        "    datum = artikel.find('metadaten/quelle/datum')\n",
        "    datum = datum.text if datum is not None else None\n",
        "\n",
        "\n",
        "    # Access variable data by their xpath \n",
        "    ressort_elem = artikel.find('inhalt/titel-liste/ressort')\n",
        "    # Store data unless data is not available and None, then store as None \n",
        "    ressort = ressort_elem.text if ressort_elem is not None else None \n",
        "\n",
        "    titel_elem = artikel.find('inhalt/titel-liste/titel')\n",
        "    titel = titel_elem.text if titel_elem is not None else None \n",
        "\n",
        "    untertitel_elem = artikel.find('inhalt/titel-liste/untertitel')\n",
        "    untertitel = untertitel_elem.text if untertitel_elem is not None else None\n",
        "\n",
        "    # Create list for text inputs \n",
        "    text = []\n",
        "    # Find the 'text' element\n",
        "    text_elem = artikel.find('inhalt/text')\n",
        "    try: \n",
        "        # Extract all the 'p' elements inside the 'text' element\n",
        "        p_elems = text_elem.findall('p')\n",
        "        # Loop over the 'p' elements and extract their text content\n",
        "        for p_elem in p_elems:\n",
        "            p_text = p_elem.text\n",
        "            # Only add text if text is not empty \n",
        "            if p_text is not None: \n",
        "              text.append(p_text)\n",
        "\n",
        "    # If no text element exists, pass \n",
        "    except: \n",
        "        pass \n",
        "\n",
        "    # Create temporary dict to store all information \n",
        "    temp_dict = {}\n",
        "    temp_dict['artikel_id'] = str(artikel_id)\n",
        "    temp_dict['name'] = name\n",
        "    temp_dict['jahrgang'] = jahrgang\n",
        "    temp_dict['datum'] = datum\n",
        "    temp_dict['ressort'] = ressort\n",
        "    temp_dict['titel'] = titel\n",
        "    temp_dict['untertitel'] = untertitel\n",
        "    temp_dict['text'] = text\n",
        "\n",
        "    # Add the article dict to the output list \n",
        "    json_file.append(temp_dict)\n",
        "  return json_file\n"
      ],
      "metadata": {
        "id": "B7NRwFGc6Bsh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_complete(prefix):\n",
        "  \"\"\" function to compare if json contains all available articles\n",
        "  Parameters:\n",
        "    - prefix (str): prefix of the journal that is checked for completeness\n",
        "  Returns:\n",
        "    - tuple (boolean, DataFrame)\n",
        "      - boolean indicates weather or not the json file is complete\n",
        "      - DataFrame returns the json data if its complete and None if incomplete\n",
        "  \"\"\"\n",
        "  try:\n",
        "    df=pd.read_json(os.path.join(\"json\",prefix+\".json\"))\n",
        "    # compare size of dataframe to number of articles\n",
        "    if len(df)==art_per_src[prefix]:\n",
        "      return (True,df)\n",
        "    else:\n",
        "      print(f\"Number of articles in {prefix} json should be {art_per_src[prefix]} but is {len(df)}.\")\n",
        "      return (False, None)\n",
        "  except:\n",
        "    return (False, None)"
      ],
      "metadata": {
        "id": "-9BTegu53TaL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting Json Files\n",
        "In this step all prefixes are looped through. Before parsing the xmls files, it is checked if a json file for that prefix already exists and if that files containes all articles. If the json files was previously exported and contains all articles, it is skipped. "
      ],
      "metadata": {
        "id": "EWH914I31Vhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load packages and set directories \n",
        "import os \n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET \n",
        "from tqdm import tqdm # for progress bar \n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set up paths for input and output directories \n",
        "input_dir = \"unzipped\"\n",
        "output_dir = \"json\"\n"
      ],
      "metadata": {
        "id": "8QqNHp-uCmEm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "T8rv7JS72QRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary saving the number of articles per prefix usingthe xml names eg. MIB_250001_260000.xml\n",
        "# this dict is use in testing method check_if_complete, this cell needs to be executed before testing\n",
        "\n",
        "art_per_src={}\n",
        "# list of all prefixes\n",
        "prefixes= sorted([i.split(\".\")[0] for i in os.listdir(\"Raw\")])\n",
        "# list of all xml files\n",
        "xmls=os.listdir(\"unzipped\")\n",
        "for prefix in prefixes:\n",
        "  # list of number of articles of prefix by title name\n",
        "  n_art=sorted([int(re.split(\"_|\\.\",xml)[-2]) for xml in xmls if xml.startswith(prefix)])\n",
        "  # save the largest number (total number of articles pf that prefix) or 0 if no xml of that prefix present\n",
        "  n_art = n_art[-1] if len(n_art)>0 else 0\n",
        "  art_per_src[prefix]=n_art\n",
        "# eg. prefix ANN has 352'684 articles\n",
        "art_per_src[\"AAN\"]"
      ],
      "metadata": {
        "id": "DuROSbI73ZiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9044fe-e0a1-4a76-dda3-f0f58e99eaf6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "352684"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list with all prefixes\n",
        "prefixes= sorted([i.split(\".\")[0] for i in os.listdir(\"Raw\")])"
      ],
      "metadata": {
        "id": "soKTh8Rn2w5I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looping through all prefixes\n",
        "for prefix in tqdm(prefixes):\n",
        "  # if a json files already exists and its complete, it is skipped\n",
        "  if not check_if_complete(prefix)[0]:\n",
        "    # list all xmls of a prefix\n",
        "    xmls=[i for i in os.listdir(input_dir) if i.startswith(prefix)]\n",
        "    # create an empty list for the json files\n",
        "    json_temp=[]\n",
        "    # loop through all xmlfile\n",
        "    for xml in xmls:\n",
        "      # parse each xml\n",
        "      json_temp=json_temp+parse_xml_file(xml)\n",
        "    #create output name  \n",
        "    output_path=os.path.join(output_dir, f\"{prefix}.json\")\n",
        "    with open(output_path, \"w\") as f:\n",
        "      # save json \n",
        "      json.dump(json_temp, f)\n",
        "  else:\n",
        "    # if json file already exists andis complete, prefix is skipped\n",
        "    print(f\"{prefix} already exported\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my9Pn_kC66bV",
        "outputId": "2f1642bb-4559-4b82-8a9c-92540c01bcbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 1/3 [00:21<00:43, 21.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAN already exported\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:26<00:11, 11.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AARB already exported\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:41<00:00, 13.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAZ already exported\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## to do list:\n",
        "\n",
        "\n",
        "*  **convert xml to json file**\n",
        "\n",
        "* **do filtering: german newspapers only, DA related topics only**\n",
        "\n",
        "* do descriptive analysis: number of articles per newspaper, number of newspapers, number of topics per newspaper, etc.\n",
        "\n",
        "* do a collocation analysis: see gitub repo \"newspaper\" under scripts\n",
        "\n",
        "* topic analysis: run the filtered dataset through a generic topic model\n"
      ],
      "metadata": {
        "id": "DBwDGU-Jy-sC"
      }
    }
  ]
}
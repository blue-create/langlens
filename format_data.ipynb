{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYpEw3y7J3q5i5qzP6t2nG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blue-create/langlens/blob/main/format_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose\n",
        "\n",
        "This file shows the steps we took to process the raw data files, zipped xml files."
      ],
      "metadata": {
        "id": "8BbjDacn_sZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect with Google drive to access data\n",
        "\n",
        "In order to access the data, you first need to create a shortcut of the data folder to your own Gdrive. If you've been granted editing rights, you should be able to edit the content of the folder, i.e. add, move and delete data, create and rename folders, etc."
      ],
      "metadata": {
        "id": "tSBVVOc8_6DR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiZS90YvWYBt",
        "outputId": "f389f4c0-8a19-4a39-b751-9b71c0711487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# redirect the working directory of this script to the data folder\n",
        "%cd /content/drive/MyDrive/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdHvzLHiXTk4",
        "outputId": "31028875-f202-45ae-8f58-9160f0211eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check number of files \n",
        "import os \n",
        "\n",
        "num_files = len(os.listdir(\".\"))\n",
        "print(\"Number of files in the folder: \", num_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDLAdk9nWvNY",
        "outputId": "59d2458d-e855-4e60-ad3d-3edb5b9d2c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the folder:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the files \n",
        "\n",
        "The first step was to unzip the files and to move the unzipped xml files to another folder called \"unzipped\". Next, we check whether any zip file in the \"raw\" data folder is missing a corresponding xml file in the \"unzipped\" folder. "
      ],
      "metadata": {
        "id": "Sqh8TaqvAayx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip all files \n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Path to the raw folder\n",
        "raw_folder = \"./Raw\"\n",
        "\n",
        "# Path to the unzipped folder\n",
        "unzipped_folder = \"./unzipped\"\n",
        "\n",
        "# Loop through all the files in the raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Check if the file is a valid zip file\n",
        "        if zipfile.is_zipfile(os.path.join(raw_folder, filename)):\n",
        "            # Create a ZipFile object for the current zip file\n",
        "            with zipfile.ZipFile(os.path.join(raw_folder, filename), \"r\") as zip_ref:\n",
        "                # Extract all the contents of the zip file to the unzipped folder\n",
        "                zip_ref.extractall(unzipped_folder)\n",
        "        else:\n",
        "            print(f\"File {filename} is not a valid zip file and will not be extracted.\")"
      ],
      "metadata": {
        "id": "E6QhI0PoVFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether a zip file is missing a xml file \n",
        "\n",
        "# Loop through all the files in the Raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Get the first part of the filename before the .zip extension\n",
        "        name = filename.split(\".\")[0]\n",
        "\n",
        "        # Loop through all the files in the unzipped folder\n",
        "        for xml_filename in os.listdir(unzipped_folder):\n",
        "            if xml_filename.startswith(name) and xml_filename.endswith(\".xml\"):\n",
        "                break\n",
        "        else:\n",
        "            # A corresponding XML file doesn't exist\n",
        "            print(f\"No corresponding XML file exists for {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS_uVeg2pl1G",
        "outputId": "adc61d5d-47ae-4dcd-aa21-c36a91f722a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No corresponding XML file exists for NBPC.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert xml files to json files \n",
        "\n",
        "For further analysis, we decided to convert the xml files to json files for the following reasons: \n",
        "\n",
        "- we are dealing with big datasets \n",
        "- CSVs are slow to query and difficult to store efficiently \n",
        "- JSON supports more complex data structures"
      ],
      "metadata": {
        "id": "rAVGMAatBEUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load packages and set directories \n",
        "\n",
        "import pandas\n",
        "import xml.etree.ElementTree as ET \n",
        "from tqdm import tqdm # for progress bar \n",
        "\n",
        "# Set up paths for input and output directories \n",
        "input_dir = \"unzipped\"\n",
        "output_dir = \"json\"\n"
      ],
      "metadata": {
        "id": "8QqNHp-uCmEm"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over all the xml files in the directory\n",
        "for xml_file in tqdm(os.listdir(input_dir)): \n",
        "  # Check if the file is an XML file\n",
        "  if xml_file.endswith(\".xml\"):\n",
        "    # Parse the xml file\n",
        "    tree = ET.parse(os.path.join(input_dir, xml_file))\n",
        "    # Get the root \n",
        "    root = tree.getroot()\n",
        "    # Create list for converted output \n",
        "    json_file = []\n",
        "    # Loop through each article, get the data and append it to the output file json_file\n",
        "    for artikel in root.findall('artikel'):\n",
        "\n",
        "      # Access static data by their xpath\n",
        "      artikel_id = artikel.find('metadaten/artikel-id').text\n",
        "      name = artikel.find('metadaten/quelle/name').text\n",
        "      jahrgang = artikel.find('metadaten/quelle/jahrgang').text\n",
        "      datum = artikel.find('metadaten/quelle/datum').text\n",
        "\n",
        "      # Access variable data by their xpath \n",
        "      ressort_elem = artikel.find('inhalt/titel-liste/ressort')\n",
        "      # Store data unless data is not available and None, then store as None \n",
        "      ressort = ressort_elem.text if ressort_elem is not None else None \n",
        "\n",
        "      titel_elem = artikel.find('inhalt/titel-liste/titel')\n",
        "      titel = titel_elem.text if titel_elem is not None else None \n",
        "\n",
        "      untertitel_elem = artikel.find('inhalt/titel-liste/untertitel')\n",
        "      untertitel = untertitel_elem.text if untertitel_elem is not None else None\n",
        "\n",
        "      # Create list for text inputs \n",
        "      text = []\n",
        "      # Find the 'text' element\n",
        "      text_elem = artikel.find('inhalt/text')\n",
        "      try: \n",
        "          # Extract all the 'p' elements inside the 'text' element\n",
        "          p_elems = text_elem.findall('p')\n",
        "          # Loop over the 'p' elements and extract their text content\n",
        "          for p_elem in p_elems:\n",
        "              p_text = p_elem.text\n",
        "              # Only add text if text is not empty \n",
        "              if p_text is not None: \n",
        "                text.append(p_text)\n",
        "\n",
        "      # If no text element exists, pass \n",
        "      except: \n",
        "          pass \n",
        "\n",
        "      # Create temporary dict to store all information \n",
        "      temp_dict = {}\n",
        "      temp_dict['artikel_id'] = artikel_id\n",
        "      temp_dict['name'] = name\n",
        "      temp_dict['jahrgang'] = jahrgang\n",
        "      temp_dict['datum'] = datum\n",
        "      temp_dict['ressort'] = ressort\n",
        "      temp_dict['titel'] = titel\n",
        "      temp_dict['untertitel'] = untertitel\n",
        "      temp_dict['text'] = text\n",
        "\n",
        "      # Add the article dict to the output list \n",
        "      json_file.append(temp_dict)\n",
        "\n",
        "    # Extract the prefix of the file name\n",
        "    prefix = xml_file.split(\"_\")[0]\n",
        "  \n",
        "    # Create the output file name\n",
        "    output_file = os.path.join(output_dir, f\"{prefix}.json\")\n",
        "        \n",
        "    # Check if the output file already exists\n",
        "    if os.path.exists(output_file):\n",
        "        # Read the existing JSON data from the output file\n",
        "        with open(output_file, \"r\") as f:\n",
        "            json_data = json.load(f)\n",
        "    else:\n",
        "        # Create a new empty JSON data dictionary\n",
        "        json_data = {\"data\": []}\n",
        "        \n",
        "    # Append the XML data to the JSON data\n",
        "    json_data[\"data\"].append(json_file)\n",
        "        \n",
        "    # Write the updated JSON data to the output file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(json_data, f)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDXtSI1kDOZ3",
        "outputId": "f6a4cb1d-36c1-4ce0-d09a-5ff56e77eff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 30/5934 [07:16<36:36:34, 22.32s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "# Set up the paths for the input and output files\n",
        "input_file_path = \"./unzipped/OSZ_1_10000.xml\"\n",
        "output_file_path = \"./json/OSZ_1_10000.json\"\n",
        "\n",
        "# Parse the XML file\n",
        "tree = ET.parse(input_file_path)\n",
        "root = tree.getroot()"
      ],
      "metadata": {
        "id": "JqvgN6MjzBjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_tab = []\n",
        "for artikel in root.findall('artikel'):\n",
        "\n",
        "    artikel_id = artikel.find('metadaten/artikel-id').text\n",
        "    name = artikel.find('metadaten/quelle/name').text\n",
        "    jahrgang = artikel.find('metadaten/quelle/jahrgang').text\n",
        "    datum = artikel.find('metadaten/quelle/datum').text\n",
        "\n",
        "    ressort_elem = artikel.find('inhalt/titel-liste/ressort')\n",
        "    ressort = ressort_elem.text if ressort_elem is not None else None \n",
        "\n",
        "    titel_elem = artikel.find('inhalt/titel-liste/titel')\n",
        "    titel = titel_elem.text if titel_elem is not None else None \n",
        "\n",
        "    untertitel_elem = artikel.find('inhalt/titel-liste/untertitel')\n",
        "    untertitel = untertitel_elem.text if untertitel_elem is not None else None\n",
        "\n",
        "    text = []\n",
        "    # Find the 'text' element\n",
        "    text_elem = artikel.find('inhalt/text')\n",
        "    try: \n",
        "        # Extract all the 'p' elements inside the 'text' element\n",
        "        p_elems = text_elem.findall('p')\n",
        "        # Loop over the 'p' elements and extract their text content\n",
        "        for p_elem in p_elems:\n",
        "            p_text = p_elem.text\n",
        "            # only add text if text is not empty \n",
        "            if p_text is not None: \n",
        "              text.append(p_text)\n",
        "    except: \n",
        "        pass \n",
        "\n",
        "    temp_dict = {}\n",
        "    temp_dict['artikel_id'] = artikel_id\n",
        "    temp_dict['name'] = name\n",
        "    temp_dict['jahrgang'] = jahrgang\n",
        "    temp_dict['datum'] = datum\n",
        "    temp_dict['ressort'] = ressort\n",
        "    temp_dict['titel'] = titel\n",
        "    temp_dict['untertitel'] = untertitel\n",
        "    temp_dict['text'] = text\n",
        "\n",
        "    json_tab.append(temp_dict)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "TDOERE4Q2-pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# serialize the dictionary to JSON\n",
        "json_data = json.dumps(json_tab, ensure_ascii=False)\n",
        "\n",
        "# write the JSON data to a file\n",
        "with open(output_file_path, \"w\") as outfile:\n",
        "    outfile.write(json_data)"
      ],
      "metadata": {
        "id": "G5DqzGtc6J55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import xmltodict\n",
        "import json\n",
        "\n",
        "# Set up the paths to the input and output directories\n",
        "input_dir = \"unzipped\"\n",
        "output_dir = \"json\"\n",
        "\n",
        "# Get a list of all the XML files in the input directory\n",
        "xml_files = glob.glob(os.path.join(input_dir, \"*.xml\"))\n",
        "\n",
        "xml_dict_by_prefix = {}\n",
        "# Loop over the XML files\n",
        "for xml_file in tqdm(xml_files):\n",
        "    # Parse the XML file to a Python dictionary using xmltodict\n",
        "    with open(xml_file, \"r\") as f:\n",
        "        xml_str = f.read()\n",
        "        xml_dict = xmltodict.parse(xml_str)\n",
        "\n",
        "    # Get the first 2-4 characters of the filename\n",
        "    file_prefix = os.path.basename(xml_file)[:4]\n",
        "\n",
        "    # Append the dictionary to the list corresponding to this file prefix\n",
        "    if file_prefix in xml_dict_by_prefix:\n",
        "        xml_dict_by_prefix[file_prefix].append(xml_dict)\n",
        "    else:\n",
        "        xml_dict_by_prefix[file_prefix] = [xml_dict]\n",
        "\n",
        "# Save each group of dictionaries as a separate JSON file in the output directory\n",
        "for file_prefix, xml_dict_list in xml_dict_by_prefix.items():\n",
        "    output_filename = os.path.join(output_dir, file_prefix + \".json\")\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        json.dump(xml_dict_list, f)\n"
      ],
      "metadata": {
        "id": "pXZ25tdSXEeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "old code"
      ],
      "metadata": {
        "id": "Z-X6Z42vXEyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# old \n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.DataFrame(columns=['Filename', 'Type', 'Content'])\n",
        "\n",
        "for entry in sorted(os.listdir('.')):\n",
        "    entry_path = os.path.join('.', entry)\n",
        "    if os.path.isdir(entry_path):\n",
        "        # Entry is a folder, add XML files in the folder to the dataset\n",
        "        for file in sorted(os.listdir(entry_path)):\n",
        "            if file.endswith('.xml'):\n",
        "                file_path = os.path.join(entry_path, file)\n",
        "                with open(file_path, 'r') as f:\n",
        "                    content = f.read()\n",
        "                dataset = dataset.append({'Filename': file, 'Type': 'XML', 'Content': content}, ignore_index=True)\n",
        "    elif os.path.isfile(entry_path) and entry.endswith('.xml'):\n",
        "        # Entry is an XML file, add it to the dataset\n",
        "        with open(entry_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        dataset = dataset.append({'Filename': entry, 'Type': 'XML', 'Content': content}, ignore_index=True)\n",
        "\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "id": "eNt4KPaHcpqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0499d8ee-c4cf-459a-b260-d0c8463d5c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Filename, Type, Content]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "things to do: \n",
        "- convert xml to json file\n",
        "- do some descriptive analysis: number of articles per newspaper, number of newspapers, number of topics per newspaper, etc. \n",
        "- do filtering: german newspapers only, DA related topics only \n",
        "- topic analysis: run the filtered dataset through a generic topic model "
      ],
      "metadata": {
        "id": "LownCWdKdW9g"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnQSAH5YXu+ECsfdsvP0xT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blue-create/langlens/blob/main/format_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiZS90YvWYBt",
        "outputId": "bb2f1f67-adea-41ef-c337-2a60ff7fd279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access data \n",
        "%cd /content/drive/MyDrive/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdHvzLHiXTk4",
        "outputId": "95026223-602b-4c07-c2c9-3d8e55b70971"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check number of files \n",
        "import os \n",
        "\n",
        "num_files = len(os.listdir(\".\"))\n",
        "print(\"Number of files in the folder: \", num_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDLAdk9nWvNY",
        "outputId": "59d2458d-e855-4e60-ad3d-3edb5b9d2c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the folder:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.DataFrame(columns=['Filename', 'Type', 'Content'])\n",
        "\n",
        "for entry in sorted(os.listdir('.')):\n",
        "    entry_path = os.path.join('.', entry)\n",
        "    if os.path.isdir(entry_path):\n",
        "        # Entry is a folder, add XML files in the folder to the dataset\n",
        "        for file in sorted(os.listdir(entry_path)):\n",
        "            if file.endswith('.xml'):\n",
        "                file_path = os.path.join(entry_path, file)\n",
        "                with open(file_path, 'r') as f:\n",
        "                    content = f.read()\n",
        "                dataset = dataset.append({'Filename': file, 'Type': 'XML', 'Content': content}, ignore_index=True)\n",
        "    elif os.path.isfile(entry_path) and entry.endswith('.xml'):\n",
        "        # Entry is an XML file, add it to the dataset\n",
        "        with open(entry_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        dataset = dataset.append({'Filename': entry, 'Type': 'XML', 'Content': content}, ignore_index=True)\n",
        "\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "id": "eNt4KPaHcpqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0499d8ee-c4cf-459a-b260-d0c8463d5c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Filename, Type, Content]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip all files\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Path to the raw folder\n",
        "raw_folder = \"./Raw\"\n",
        "\n",
        "# Path to the unzipped folder\n",
        "unzipped_folder = \"./unzipped\"\n",
        "\n",
        "# Loop through all the files in the raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Check if the file is a valid zip file\n",
        "        if zipfile.is_zipfile(os.path.join(raw_folder, filename)):\n",
        "            # Create a ZipFile object for the current zip file\n",
        "            with zipfile.ZipFile(os.path.join(raw_folder, filename), \"r\") as zip_ref:\n",
        "                # Extract all the contents of the zip file to the unzipped folder\n",
        "                zip_ref.extractall(unzipped_folder)\n",
        "        else:\n",
        "            print(f\"File {filename} is not a valid zip file and will not be extracted.\")"
      ],
      "metadata": {
        "id": "E6QhI0PoVFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all the files in the Raw folder\n",
        "for filename in os.listdir(raw_folder):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        # Get the first part of the filename before the .zip extension\n",
        "        name = filename.split(\".\")[0]\n",
        "\n",
        "        # Loop through all the files in the unzipped folder\n",
        "        for xml_filename in os.listdir(unzipped_folder):\n",
        "            if xml_filename.startswith(name) and xml_filename.endswith(\".xml\"):\n",
        "                break\n",
        "        else:\n",
        "            # A corresponding XML file doesn't exist\n",
        "            print(f\"No corresponding XML file exists for {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS_uVeg2pl1G",
        "outputId": "adc61d5d-47ae-4dcd-aa21-c36a91f722a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No corresponding XML file exists for NBPC.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm # for progress bar \n",
        "\n",
        "data_folder = \"unzipped\"\n",
        "prefix = \"FAZE_\"  # set the prefix you want to test here\n",
        "\n",
        "# create a list of all xml files in the data folder\n",
        "xml_files = [f for f in os.listdir(data_folder) if f.endswith('.xml')]\n",
        "\n",
        "# filter the xml files by prefix\n",
        "prefix_files = [f for f in xml_files if f.startswith(prefix)]"
      ],
      "metadata": {
        "id": "Eh0KwMMirO2I"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty pandas DataFrame to store the xml data\n",
        "xml_data = pd.DataFrame(columns=[\"name\",\n",
        "                                 \"jahrgang\", \n",
        "                                 \"datum\", \n",
        "                                 \"ressort\", \n",
        "                                 #\"titel\", \n",
        "                                 \"untertitel\", \n",
        "                                 \"text\"\n",
        "                                 ])\n",
        "\n",
        "# iterate over the xml files and extract the data\n",
        "for xml_file in tqdm(prefix_files):\n",
        "    xml_path = os.path.join(data_folder, xml_file)\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    for artikel in root.findall('artikel'):\n",
        "        name = artikel.find('metadaten/quelle/name').text\n",
        "        jahrgang = artikel.find('metadaten/quelle/jahrgang').text\n",
        "        datum = artikel.find('metadaten/quelle/datum').text\n",
        "\n",
        "        ressort = artikel.find('inhalt/titel-liste/ressort').text\n",
        "        #titel = artikel.find('inhalt/titel-liste/titel').text # does not work for some reason, needs fixing\n",
        "\n",
        "        untertitel_elem = artikel.find('inhalt/titel-liste/untertitel')\n",
        "        untertitel = untertitel_elem.text if untertitel is not None else None\n",
        "\n",
        "        text = []\n",
        "        # Find the 'text' element\n",
        "        text_elem = artikel.find('inhalt/text')\n",
        "        try: \n",
        "            # Extract all the 'p' elements inside the 'text' element\n",
        "            p_elems = text_elem.findall('p')\n",
        "            # Loop over the 'p' elements and extract their text content\n",
        "            for p_elem in p_elems:\n",
        "                p_text = p_elem.text\n",
        "                text.append(p_text)\n",
        "        except: \n",
        "            pass \n",
        "\n",
        "\n",
        "        row = pd.DataFrame({'name': name, \n",
        "               'jahrgang': jahrgang, \n",
        "               'datum': datum,\n",
        "               'ressort':ressort,\n",
        "               #'titel': titel, \n",
        "               'untertitel': untertitel, \n",
        "               'text': text})\n",
        "        xml_data = pd.concat([xml_data, row], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoxONSQF2PYw",
        "outputId": "1856e593-be42-4b2c-ee73-a8484eba9e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the Excel file for the prefix\n",
        "excel_file = os.path.join(\"excel\", f\"{prefix}data.xlsx\")\n",
        "xml_data.to_excel(excel_file, index=False)"
      ],
      "metadata": {
        "id": "wH67Z53P2nAf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "things to do: \n",
        "- connect to github \n",
        "- create and store dataset \n",
        "- do some descriptive analysis: number of articles per newspaper, number of newspapers, number of topics per newspaper, etc. \n",
        "- do filtering: german newspapers only, DA related topics only \n",
        "- topic analysis: run the filtered dataset through a generic topic model "
      ],
      "metadata": {
        "id": "LownCWdKdW9g"
      }
    }
  ]
}